{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blaine\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "import librosa\n",
    "import os as os\n",
    "from scipy.misc import comb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./large_files/filtered_audio/\"  \n",
    "\n",
    "\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n",
    "# Get available labels\n",
    "labels, indices, _ = get_labels(DATA_PATH)\n",
    "\n",
    "\n",
    "# Getting first arrays\n",
    "X = np.load(labels[0] + '.npy')\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "# Append all of the dataset into one single array, same goes for y\n",
    "for i, label in enumerate(labels[1:]):\n",
    "    x = np.load(label + '.npy')\n",
    "    X = np.vstack((X, x))\n",
    "    y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "assert X.shape[0] == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# # Feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "epochs = \n",
    "batch_size = 100\n",
    "verbose = 2\n",
    "num_classes = 31\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X = X.reshape(X.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "\n",
    "y_hot = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_larger_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(96, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(240, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 17s - loss: 2.9055 - acc: 0.1677\n",
      "Epoch 2/50\n",
      " - 6s - loss: 1.6673 - acc: 0.4962\n",
      "Epoch 3/50\n",
      " - 6s - loss: 1.2199 - acc: 0.6391\n",
      "Epoch 4/50\n",
      " - 6s - loss: 1.0176 - acc: 0.7045\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.8939 - acc: 0.7411\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.8044 - acc: 0.7699\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.7455 - acc: 0.7862\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.6998 - acc: 0.7986\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.6596 - acc: 0.8100\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.6285 - acc: 0.8181\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.5962 - acc: 0.8266\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.5752 - acc: 0.8337\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.5533 - acc: 0.8377\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.5340 - acc: 0.8449\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.5198 - acc: 0.8480\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.5094 - acc: 0.8522\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.4893 - acc: 0.8578\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.4753 - acc: 0.8623\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.4660 - acc: 0.8632\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.4596 - acc: 0.8671\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.4448 - acc: 0.8711\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.4405 - acc: 0.8728\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.4372 - acc: 0.8739\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.4301 - acc: 0.8756\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.4234 - acc: 0.8773\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.4168 - acc: 0.8797\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.4141 - acc: 0.8800\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.4157 - acc: 0.8811\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.4059 - acc: 0.8839\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.4058 - acc: 0.8838\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.4033 - acc: 0.8852\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.3996 - acc: 0.8858\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.4023 - acc: 0.8854\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.3992 - acc: 0.8857\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.4004 - acc: 0.8868\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.3939 - acc: 0.8883\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.3884 - acc: 0.8901\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.3921 - acc: 0.8892\n",
      "Epoch 39/50\n",
      " - 6s - loss: 0.3847 - acc: 0.8920\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.3886 - acc: 0.8910\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.3851 - acc: 0.8907\n",
      "Epoch 42/50\n",
      " - 6s - loss: 0.3930 - acc: 0.8910\n",
      "Epoch 43/50\n",
      " - 6s - loss: 0.3870 - acc: 0.8908\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.3837 - acc: 0.8914\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.3832 - acc: 0.8916\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.3858 - acc: 0.8921\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.3734 - acc: 0.8956\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.3802 - acc: 0.8942\n",
      "Epoch 49/50\n",
      " - 6s - loss: 0.3778 - acc: 0.8935\n",
      "Epoch 50/50\n",
      " - 6s - loss: 0.3788 - acc: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13cd139d588>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we want to use kfolds or whatever\n",
    "\n",
    "# k=2\n",
    "# num_val_samples = len(X) // k\n",
    "# all_scores = []\n",
    "\n",
    "\n",
    "# for i in range(k):\n",
    "#     print('processing fold #', i)\n",
    "#     # Prepare the validation data: data from partition # k\n",
    "#     val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#     val_targets = y_hot[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "#     # Prepare the training data: data from all other partitions\n",
    "#     partial_train_data = np.concatenate(\n",
    "#         [X[:i * num_val_samples],\n",
    "#          X[(i + 1) * num_val_samples:]],\n",
    "#         axis=0)\n",
    "#     partial_train_targets = np.concatenate(\n",
    "#         [y_hot[:i * num_val_samples],\n",
    "#          y_hot[(i + 1) * num_val_samples:]],\n",
    "#         axis=0)\n",
    "\n",
    "# (smaller model)\n",
    "small_model = get_model()\n",
    "small_model.fit(X, y_hot,\n",
    "batch_size=batch_size, \n",
    "epochs=epochs, \n",
    "verbose=verbose         )\n",
    "\n",
    "\n",
    "#     val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "#     all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 27s - loss: 2.1391 - acc: 0.3889\n",
      "Epoch 2/50\n",
      " - 11s - loss: 1.0275 - acc: 0.6998\n",
      "Epoch 3/50\n",
      " - 10s - loss: 0.7628 - acc: 0.7781\n",
      "Epoch 4/50\n",
      " - 10s - loss: 0.6297 - acc: 0.8156\n",
      "Epoch 5/50\n",
      " - 10s - loss: 0.5373 - acc: 0.8431\n",
      "Epoch 6/50\n",
      " - 10s - loss: 0.4650 - acc: 0.8626\n",
      "Epoch 7/50\n",
      " - 11s - loss: 0.4141 - acc: 0.8759\n",
      "Epoch 8/50\n",
      " - 11s - loss: 0.3682 - acc: 0.8904\n",
      "Epoch 9/50\n",
      " - 11s - loss: 0.3328 - acc: 0.9008\n",
      "Epoch 10/50\n",
      " - 11s - loss: 0.3048 - acc: 0.9092\n",
      "Epoch 11/50\n",
      " - 10s - loss: 0.2839 - acc: 0.9148\n",
      "Epoch 12/50\n",
      " - 11s - loss: 0.2636 - acc: 0.9205\n",
      "Epoch 13/50\n",
      " - 10s - loss: 0.2439 - acc: 0.9262\n",
      "Epoch 14/50\n",
      " - 10s - loss: 0.2314 - acc: 0.9313\n",
      "Epoch 15/50\n",
      " - 10s - loss: 0.2178 - acc: 0.9350\n",
      "Epoch 16/50\n",
      " - 10s - loss: 0.2113 - acc: 0.9361\n",
      "Epoch 17/50\n",
      " - 11s - loss: 0.2048 - acc: 0.9388\n",
      "Epoch 18/50\n",
      " - 11s - loss: 0.1913 - acc: 0.9428\n",
      "Epoch 19/50\n",
      " - 10s - loss: 0.1814 - acc: 0.9463\n",
      "Epoch 20/50\n",
      " - 11s - loss: 0.1743 - acc: 0.9485\n",
      "Epoch 21/50\n",
      " - 11s - loss: 0.1750 - acc: 0.9491\n",
      "Epoch 22/50\n",
      " - 11s - loss: 0.1732 - acc: 0.9494\n",
      "Epoch 23/50\n",
      " - 11s - loss: 0.1740 - acc: 0.9497\n",
      "Epoch 24/50\n",
      " - 11s - loss: 0.1678 - acc: 0.9512\n",
      "Epoch 25/50\n",
      " - 10s - loss: 0.1662 - acc: 0.9532\n",
      "Epoch 26/50\n",
      " - 10s - loss: 0.1681 - acc: 0.9530\n",
      "Epoch 27/50\n",
      " - 10s - loss: 0.1734 - acc: 0.9519\n",
      "Epoch 28/50\n",
      " - 10s - loss: 0.1706 - acc: 0.9525\n",
      "Epoch 29/50\n",
      " - 10s - loss: 0.1684 - acc: 0.9533\n",
      "Epoch 30/50\n",
      " - 10s - loss: 0.1691 - acc: 0.9536\n",
      "Epoch 31/50\n",
      " - 10s - loss: 0.1732 - acc: 0.9524\n",
      "Epoch 32/50\n",
      " - 10s - loss: 0.1729 - acc: 0.9539\n",
      "Epoch 33/50\n",
      " - 10s - loss: 0.1691 - acc: 0.9549\n",
      "Epoch 34/50\n",
      " - 11s - loss: 0.1700 - acc: 0.9549\n",
      "Epoch 35/50\n",
      " - 10s - loss: 0.1715 - acc: 0.9548\n",
      "Epoch 36/50\n",
      " - 11s - loss: 0.1724 - acc: 0.9558\n",
      "Epoch 37/50\n",
      " - 11s - loss: 0.1654 - acc: 0.9567\n",
      "Epoch 38/50\n",
      " - 10s - loss: 0.1764 - acc: 0.9545\n",
      "Epoch 39/50\n",
      " - 11s - loss: 0.1749 - acc: 0.9556\n",
      "Epoch 40/50\n",
      " - 10s - loss: 0.1756 - acc: 0.9552\n",
      "Epoch 41/50\n",
      " - 11s - loss: 0.1729 - acc: 0.9560\n",
      "Epoch 42/50\n",
      " - 10s - loss: 0.1722 - acc: 0.9558\n",
      "Epoch 43/50\n",
      " - 10s - loss: 0.1736 - acc: 0.9559\n",
      "Epoch 44/50\n",
      " - 10s - loss: 0.1804 - acc: 0.9554\n",
      "Epoch 45/50\n",
      " - 10s - loss: 0.1744 - acc: 0.9556\n",
      "Epoch 46/50\n",
      " - 10s - loss: 0.1802 - acc: 0.9550\n",
      "Epoch 47/50\n",
      " - 10s - loss: 0.1794 - acc: 0.9551\n",
      "Epoch 48/50\n",
      " - 10s - loss: 0.1775 - acc: 0.9573\n",
      "Epoch 49/50\n",
      " - 10s - loss: 0.1723 - acc: 0.9559\n",
      "Epoch 50/50\n",
      " - 11s - loss: 0.1797 - acc: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13cfffb60f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_larger_model()\n",
    "model.fit(X, y_hot,\n",
    "batch_size=batch_size, \n",
    "epochs=epochs, \n",
    "verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('good_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model.save('small_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 19, 10, 64)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 9, 96)         24672     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 17, 8, 240)        92400     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 4, 240)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 4, 240)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               1966336   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 31)                3999      \n",
      "=================================================================\n",
      "Total params: 2,120,623\n",
      "Trainable params: 2,120,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
